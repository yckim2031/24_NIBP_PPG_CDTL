{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23dd95ea-b087-4fa4-a573-363a703651fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 23:55:23.075805: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-03 23:55:23.109784: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-03 23:55:23.109810: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-03 23:55:23.109829: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-03 23:55:23.115628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-03 23:55:23.685599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84bd3b5b-d68b-4c1b-813f-11959a4fd285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Result_calculator(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        SBP_val = float(lines[1][12:-1])\n",
    "        DBP_val = float(lines[2][12:-1])\n",
    "        SBP_PR_val = float(lines[3][-4:-1])\n",
    "        DBP_PR_val = float(lines[4][-4:-1])\n",
    "        \n",
    "        return [SBP_val, DBP_val, SBP_PR_val, DBP_PR_val]\n",
    "\n",
    "# 각 환자별 평균 계산 - attempt 없애기\n",
    "def np_to_df(res_data):\n",
    "    res_by_sub = np.empty((0,5))\n",
    "    sub_list = np.unique(res_data[:,0])\n",
    "\n",
    "    for sub in sub_list:\n",
    "        temp_sub_list = np.where(res_data[:,0]==sub)\n",
    "        temp_res_val = res_data[temp_sub_list, 3:]\n",
    "        temp_mean = np.mean(temp_res_val, axis = 1)\n",
    "        temp_mean = np.insert(temp_mean, 0, sub)\n",
    "        res_by_sub = np.insert(res_by_sub, res_by_sub.shape[0], temp_mean, axis = 0)\n",
    "    \n",
    "    res_dict = {\"Subject\":res_by_sub[:,0].astype(int), \"SBP MAE\":res_by_sub[:,1], \"DBP MAE\":res_by_sub[:,2], \n",
    "                   \"SBP PR\":res_by_sub[:,3], \"DBP PR\":res_by_sub[:,4]}\n",
    "    res_pd = pd.DataFrame(res_dict)\n",
    "    res_pd = res_pd.set_index('Subject')\n",
    "\n",
    "    return res_pd\n",
    "\n",
    "def res_reader(directory_path):\n",
    "    # From model folders to Dataframe\n",
    "    \n",
    "    res_50 = np.empty((0,7), dtype = float) #axis = 3: Sub num, data_num, attempt, SBP_MAE, DBP_MAE, SBP_PR, DBP_PR\n",
    "    res_100 = np.empty((0,7), dtype = float)\n",
    "    res_360 = np.empty((0,7), dtype = float)\n",
    "    res_720 = np.empty((0,7), dtype = float)\n",
    "    res_1800 = np.empty((0,7), dtype = float)\n",
    "    \n",
    "    sub_res = np.empty((0,7), dtype = float)\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            if file.endswith(\".txt\"):\n",
    "                directory, filename = os.path.split(file_path)\n",
    "                directory, attempt = os.path.split(directory)\n",
    "                if int(attempt) < 2:\n",
    "                    directory, data_num = os.path.split(directory)\n",
    "                    directory, sub_num = os.path.split(directory)\n",
    "                    result = Result_calculator(file_path)\n",
    "                    temp_res = [sub_num, data_num, attempt]\n",
    "                    temp_res = np.append(temp_res, result)\n",
    "                    sub_res = np.insert(sub_res, sub_res.shape[0], temp_res, axis = 0)\n",
    "    \n",
    "    # 하나의 넘파이 배열 안에 모든 데이터 넣기 - attempt 별 데이터 모두 들어감\n",
    "    for data in sub_res:\n",
    "        if int(data[1]) == 50:\n",
    "            res_50 = np.insert(res_50, res_50.shape[0], data, axis = 0)\n",
    "        if int(data[1]) == 100:\n",
    "            res_100 = np.insert(res_100, res_100.shape[0], data, axis = 0)\n",
    "        if int(data[1]) == 360:\n",
    "            res_360 = np.insert(res_360, res_360.shape[0], data, axis = 0)\n",
    "        if int(data[1]) == 720:\n",
    "            res_720 = np.insert(res_720, res_720.shape[0], data, axis = 0)\n",
    "        if int(data[1]) == 1800:\n",
    "            res_1800 = np.insert(res_1800, res_1800.shape[0], data, axis = 0)\n",
    "\n",
    "    return res_50, res_100, res_360, res_720, res_1800\n",
    "    \n",
    "def group_res(dir_path):\n",
    "    res_50_df, res_100_df, res_360_df, res_720_df, res_1800_df = res_reader(dir_path)\n",
    "    \n",
    "    _50 = round(np.mean(res_50_df,axis = 0),2)\n",
    "    _100 = round(np.mean(res_100_df,axis = 0),2)\n",
    "    _360 = round(np.mean(res_360_df,axis = 0),2)\n",
    "    _720 = round(np.mean(res_720_df,axis = 0),2)\n",
    "    _1800 = round(np.mean(res_1800_df,axis = 0),2)\n",
    "    \n",
    "    res_df = pd.concat((_50, _100, _360, _720, _1800), axis = 1).T\n",
    "    res_df.index = [50, 100, 360, 720, 1800]\n",
    "\n",
    "    return res_df\n",
    "\n",
    "def load_dataset(data_path_list):\n",
    "    x_data, y_data = np.empty((0,125)), np.empty((0,2))\n",
    "    for data_dir_list in data_path_list:\n",
    "        file_list = os.listdir(data_dir_list)\n",
    "        for file_name in file_list:\n",
    "            if file_name.endswith('.npz'):\n",
    "                data = np.load(data_dir_list+\"//\"+file_name)\n",
    "                x_data = np.concatenate((x_data, data['x'][:3240]),axis=0)\n",
    "                y_data = np.concatenate((y_data, data['y'][:3240]),axis=0)\n",
    "    return x_data, y_data\n",
    "\n",
    "def find_files(folder_path, kind):\n",
    "    file_root_list = []\n",
    "    for (path, dir, files) in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(kind):\n",
    "                file_root = os.path.join(path, filename)\n",
    "                file_root_list.append(file_root)\n",
    "    return file_root_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef1085-191d-4033-94ac-6ee1acbdfc78",
   "metadata": {},
   "source": [
    "### BHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2a42d103-1f3a-4d76-9a05-6f307d6d1c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBP\n",
      "<= 5mmHg : 0.73\n",
      "<= 10mmHg : 0.93\n",
      "<= 15mmHg : 0.98\n",
      "DBP\n",
      "<= 5mmHg : 0.91\n",
      "<= 10mmHg : 0.99\n",
      "<= 15mmHg : 1.0\n"
     ]
    }
   ],
   "source": [
    "# BHS\n",
    "dir_path = '/home/yckim/research/nibp_ppg/ex_3/model/transfer/v1_to_v2/'\n",
    "sub_list = find_files(dir_path, \"used_data_1.npz\")\n",
    "error = np.empty((0,2))\n",
    "\n",
    "for sub in sub_list:\n",
    "    a = os.path.dirname(sub)\n",
    "    b = os.path.dirname(a)\n",
    "    tl_data_number = int(os.path.split(b)[1])\n",
    "    \n",
    "    if tl_data_number == 1800:\n",
    "        \n",
    "        model_path = find_files(a, \".h5\")\n",
    "        model = load_model(model_path[0])\n",
    "        data = np.load(sub)\n",
    "        x_test, y_test = data['x_test'], data['y_test']\n",
    "\n",
    "        y_pred = model.predict(x_test, verbose = 0)\n",
    "        error = np.insert(error, error.shape[0], abs(y_test - y_pred), axis = 0)\n",
    "\n",
    "less_5mmHg_pnt = round(np.count_nonzero(error[:,0]<= 5)/error.shape[0],2)\n",
    "less_10mmHg_pnt = round(np.count_nonzero(error[:,0]<= 10)/error.shape[0],2)\n",
    "less_15mmHg_pnt = round(np.count_nonzero(error[:,0]<= 15)/error.shape[0],2)\n",
    "\n",
    "print(f\"SBP\\n<= 5mmHg : {less_5mmHg_pnt}\\n<= 10mmHg : {less_10mmHg_pnt}\\n<= 15mmHg : {less_15mmHg_pnt}\")\n",
    "\n",
    "less_5mmHg_pnt = round(np.count_nonzero(error[:,1]<= 5)/error.shape[0],2)\n",
    "less_10mmHg_pnt = round(np.count_nonzero(error[:,1]<= 10)/error.shape[0],2)\n",
    "less_15mmHg_pnt = round(np.count_nonzero(error[:,1]<= 15)/error.shape[0],2)\n",
    "\n",
    "print(f\"DBP\\n<= 5mmHg : {less_5mmHg_pnt}\\n<= 10mmHg : {less_10mmHg_pnt}\\n<= 15mmHg : {less_15mmHg_pnt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2310",
   "language": "python",
   "name": "ml2310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
