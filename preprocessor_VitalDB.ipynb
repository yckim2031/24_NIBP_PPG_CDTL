{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c82e6f",
   "metadata": {},
   "source": [
    "VitalDB's blood pressure signal has noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc41a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import cheby2, freqz, filtfilt\n",
    "from scipy.signal import decimate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41de8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def Nan_interpolation(input_sig):\n",
    "    nan_indices = np.isnan(input_sig)\n",
    "    non_nan_indices = np.arange(len(input_sig))[~nan_indices]\n",
    "    \n",
    "    input_sig[nan_indices] = np.interp(np.where(nan_indices)[0], non_nan_indices, input_sig[non_nan_indices])\n",
    "    \n",
    "    return input_sig\n",
    "\n",
    "#FIR BPF\n",
    "def BPFilter(input_sig, lowcut=0.5, highcut=5.0, numtaps = 1001, window = 'hamming'):\n",
    "    # Define filter parameters\n",
    "    fs = 125 # Sampling frequency (Hz)\n",
    "    nyquist = 0.5 * fs # Nyquist frequency\n",
    "    bands = [lowcut/nyquist, highcut/nyquist]\n",
    "\n",
    "    # Design the ppg FIR filter\n",
    "    fir_coeff = signal.firwin(numtaps, bands, window = window, pass_zero = 'bandpass')\n",
    "\n",
    "    # Apply the filter to the PPG signal\n",
    "    filtered_sig = signal.lfilter(fir_coeff, 1.0, input_sig)[int((numtaps-1)/2):] #Filter delay\n",
    "    \n",
    "    return filtered_sig\n",
    "\n",
    "def MAFilter(input_sig, window_size):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    smoothed_signal = np.convolve(input_sig, window, mode='same')\n",
    "    return smoothed_signal\n",
    "\n",
    "def SAI_Algorithm(input_sig):\n",
    "    # 1. Beat detection\n",
    "    peaks_indx, _ = signal.find_peaks(input_sig, distance = 60)\n",
    "    valleys_indx = np.array([], dtype=np.int64)\n",
    "    valleys_indx = np.append(valleys_indx, [np.argmin(input_sig[peaks_indx[i]: peaks_indx[i + 1]]) + peaks_indx[i] for i in range(len(peaks_indx) - 1)])\n",
    "    \n",
    "    # 3. Abnormality criterion\n",
    "    Ps_c = 180\n",
    "    Pd_c = 20\n",
    "    Pm_min_c, Pm_max_c = 30, 200\n",
    "    f_min_c, f_max_c = 20, 200\n",
    "    Pp_c = 20\n",
    "    T = 2 #second\n",
    "    dPs_c = 20\n",
    "    dPd_c = 20\n",
    "    dT_c = 2/3\n",
    "    \n",
    "    bp_ft_Ps = input_sig[peaks_indx[2:-1]]\n",
    "    bp_ft_Pd = input_sig[valleys_indx[2:]]\n",
    "    bp_ft_Pp = bp_ft_Ps - bp_ft_Pd\n",
    "    bp_ft_Pm = (bp_ft_Ps + bp_ft_Pd) / 3\n",
    "    bp_ft_T = (valleys_indx[2:] - valleys_indx[1:-1]) / 125\n",
    "    bp_ft_f = 60 / bp_ft_T\n",
    "    \n",
    "    dPs = np.abs(bp_ft_Ps - input_sig[peaks_indx[1:-2]])\n",
    "    dPd = np.abs(bp_ft_Pd - input_sig[valleys_indx[1:-1]])\n",
    "    dT = np.abs(bp_ft_T - (valleys_indx[1:-1] - valleys_indx[:-2]) / 125)\n",
    "    \n",
    "    mask = (\n",
    "        (bp_ft_Ps > Ps_c) |\n",
    "        (bp_ft_Ps < 80) |\n",
    "        (bp_ft_Pd < Pd_c) |\n",
    "        (bp_ft_Pm < Pm_min_c) | (bp_ft_Pm > Pm_max_c) |\n",
    "        (bp_ft_Pp < Pp_c) |\n",
    "        (bp_ft_f < f_min_c) | (bp_ft_f > f_max_c) |\n",
    "        (dPs > dPs_c) |\n",
    "        (dPd > dPd_c) |\n",
    "        (dT > dT_c) |\n",
    "        (bp_ft_T > T)\n",
    "    )\n",
    "    \n",
    "    corrupted_indx = valleys_indx[2:][mask]\n",
    "            \n",
    "    return corrupted_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30004dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type Raw dataset folder:  /home/yoonchul/Downloads/Preprocessed\n",
      "Type Save dataset folder:  /home/yoonchul/Downloads/Preprocessed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vitaldb_raw_data_p1347.npz successfully processed, ppg data size:  (3252, 125) blood pressure size:  (3252, 2)\n"
     ]
    }
   ],
   "source": [
    "#Data name\n",
    "raw_data_dir = input(\"Type Raw dataset folder: \")\n",
    "down_folder_dir = input(\"Type Save dataset folder: \")\n",
    "\n",
    "file_list = os.listdir(raw_data_dir)\n",
    "for file_name in file_list:\n",
    "    if file_name.endswith('.npz'):\n",
    "\n",
    "        #Load Data\n",
    "        Data = np.load(raw_data_dir+\"/\"+file_name)\n",
    "        ppg_raw_sig = Data['x']\n",
    "        bp_raw_sig = Data['y']\n",
    "        \n",
    "        #Nan 제거\n",
    "        ppg_raw_sig = Nan_interpolation(ppg_raw_sig)\n",
    "        bp_raw_sig = Nan_interpolation(bp_raw_sig)\n",
    "\n",
    "        #Lag compensation using Cross Correlation function\n",
    "        ppg_fp = ppg_raw_sig[0:250]\n",
    "        ppg_fp = (ppg_fp-np.mean(ppg_fp))/np.std(ppg_fp)\n",
    "        bp_fp = bp_raw_sig[0:250]\n",
    "        bp_fp = (bp_fp-np.mean(bp_fp))/np.std(bp_fp)\n",
    "        correlation = np.correlate(ppg_fp, bp_fp, mode='same')\n",
    "        max_indx = signal.find_peaks(correlation[125:])\n",
    "        ppg_sig_lc = ppg_raw_sig[max_indx[0][0]:]\n",
    "        bp_raw_sig = bp_raw_sig[:-max_indx[0][0]]\n",
    "\n",
    "        #Frequency filter\n",
    "        # FIR Filter 사용\n",
    "        numtaps = 1001\n",
    "        #ppg_BPF_sig = BPFilter(ppg_sig_lc, numtaps = numtaps, window = 'hamming')\n",
    "        ppg_sig_lc = ppg_sig_lc[(numtaps-1)//2:]\n",
    "        bp_MAF_sig = MAFilter(bp_raw_sig, 10)[(numtaps-1)//2:]\n",
    "\n",
    "        #Downsample to 25Hz\n",
    "        ori_rate = 125\n",
    "        targ_rate = 25\n",
    "        decimation_factor = int(ori_rate / targ_rate)\n",
    "        ppg_sig = decimate(ppg_sig_lc, decimation_factor, zero_phase=True)\n",
    "\n",
    "        #5초간 윈도우\n",
    "        indx_num_ppg = 5 * 25\n",
    "        num_chunks_ppg = ppg_sig.shape[0] // indx_num_ppg\n",
    "\n",
    "        indx_num_bp = 5 * 125\n",
    "        num_chunks_bp = bp_MAF_sig.shape[0] // indx_num_bp\n",
    "\n",
    "        ppg_data = ppg_sig[:num_chunks_ppg * indx_num_ppg].reshape(num_chunks_ppg, indx_num_ppg)\n",
    "        bp_data = bp_MAF_sig[:num_chunks_bp * indx_num_bp].reshape(num_chunks_bp, indx_num_bp)\n",
    "\n",
    "        #Normalization\n",
    "        ppg_data_mean = np.mean(ppg_data, axis=1, keepdims=True)\n",
    "        ppg_data_std = np.std(ppg_data, axis=1, keepdims=True)\n",
    "        ppg_data = (ppg_data - ppg_data_mean) / ppg_data_std\n",
    "\n",
    "        #Auto-correlation - ppg_corrupted_sig - Auto-correlation 값으로 줄세워서 가장 주기성 높은 애들로만 7200개 모으기\n",
    "        #auto_correlation 배열에 두번째 correlation peak 값 저장\n",
    "        ppg_data_corrupted_indx = np.empty((0,), dtype=int)\n",
    "        auto_correlation = np.empty((0,))\n",
    "\n",
    "        for i in range(ppg_data.shape[0]):\n",
    "            auto_correlation_array = np.correlate(ppg_data[i],ppg_data[i],mode='same')\n",
    "            auto_correlation_array = auto_correlation_array/np.max(auto_correlation_array)\n",
    "            temp_peaks = signal.find_peaks(auto_correlation_array, height=0.7)\n",
    "\n",
    "            if temp_peaks[0].shape[0] < 3 or temp_peaks[0].shape[0] > 5:\n",
    "                ppg_data_corrupted_indx = np.append(ppg_data_corrupted_indx, i)\n",
    "                auto_correlation = np.append(auto_correlation,0)\n",
    "            else:\n",
    "                sec_peak_indx = temp_peaks[0][temp_peaks[0].shape[0]//2+1]\n",
    "                auto_correlation = np.append(auto_correlation, auto_correlation_array[sec_peak_indx])\n",
    "\n",
    "        #BP SAI corrupted signal filter\n",
    "        bp_corrupted_indx = SAI_Algorithm(bp_MAF_sig)\n",
    "\n",
    "        bp_data_corrupted_indx = np.empty((0,), dtype = int)\n",
    "        for i in range(bp_data.shape[0]):\n",
    "            ispeak, _ = signal.find_peaks(bp_data[i], distance = 60)\n",
    "            if len(ispeak) <= 2:\n",
    "                bp_data_corrupted_indx = np.append(bp_data_corrupted_indx, i)\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                for k in range(bp_corrupted_indx.shape[0]):\n",
    "                    if bp_corrupted_indx[k]>625*i and bp_corrupted_indx[k]<625*(i+1):\n",
    "                        bp_data_corrupted_indx = np.append(bp_data_corrupted_indx, i)\n",
    "                        break\n",
    "\n",
    "        corrupted_data_indx = np.concatenate((ppg_data_corrupted_indx, bp_data_corrupted_indx))\n",
    "        corrupted_data_indx = np.unique(corrupted_data_indx)\n",
    "\n",
    "        ppg_data = np.delete(ppg_data, corrupted_data_indx, axis=0)\n",
    "        bp_data = np.delete(bp_data, corrupted_data_indx, axis=0)\n",
    "        auto_correlation = np.delete(auto_correlation, corrupted_data_indx, axis=0)\n",
    "        auto_correlation_indx = np.argsort(auto_correlation)[::-1]\n",
    "\n",
    "        #Sort according to correlation value index\n",
    "        ppg_data = ppg_data[auto_correlation_indx]\n",
    "        bp_data = bp_data[auto_correlation_indx]\n",
    "\n",
    "        #Calculate mean of blood pressures\n",
    "        bp_data_label = np.empty((0,2))\n",
    "        for i in range(bp_data.shape[0]):\n",
    "            peaks_temp,_ = signal.find_peaks(bp_data[i], distance = 60)\n",
    "            valleys_temp = np.empty((0,),dtype=int)\n",
    "            valleys_temp = np.append(valleys_temp, [np.argmin(bp_data[i, peaks_temp[k]: peaks_temp[k + 1]]) + peaks_temp[k] for k in range(len(peaks_temp) - 1)])\n",
    "            peaks_mean = np.mean(bp_data[i,peaks_temp])\n",
    "            valleys_mean = np.mean(bp_data[i, valleys_temp])\n",
    "            bp_data_label = np.insert(bp_data_label, bp_data_label.shape[0], [peaks_mean, valleys_mean], axis=0)\n",
    "        \n",
    "        if bp_data_label.shape[0]<1000:\n",
    "            print(file_name + \" has no enough data left\")\n",
    "            \n",
    "        #처리한 데이터 저장\n",
    "        print(file_name+\" successfully processed, ppg data size: \", ppg_data.shape, 'blood pressure size: ', bp_data_label.shape)\n",
    "        npz_filename = \"processed_\"+file_name[:8]+file_name[17:]\n",
    "        np.savez(down_folder_dir+\"/\"+npz_filename, x=ppg_data, y=bp_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b18d201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'F:\\Lab\\Research\\NIBP_PPG\\Dataset\\Ex_3\\Vitaldb_dataset\\Preprocessed'\n",
    "file_path = folder_path+input(\"Type filename: \")\n",
    "data = np.load(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
